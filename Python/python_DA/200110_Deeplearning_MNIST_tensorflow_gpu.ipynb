{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 설치해야 함\n",
    "# pip install pandas\n",
    "# pip install numpy\n",
    "# pip install matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pip install tensorflow-gpu==1.15 // default: cpu version\n",
    "# cuda, cudnn, tensorflow의 버전이 모두 같아야 함!!\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인공지능(AI)\n",
    "# CS에서 궁극의 목표 중 하나\n",
    "# 문제도 많음  ex) 직업적인 문제\n",
    "# 20년 뒤에는 직업의 절반가량이, 거기서 10년 정도 더 지나면 모든 직업이 사라진다는 예측이 있음\n",
    "# 사람은 무엇을 하며 살아야 하는가?\n",
    "\n",
    "# 빅뱅의 시작을 1년으로 잡으면 인류의 탄생은 2일 전\n",
    "# 산업혁명은 2초 전\n",
    "# 기술의 발전 속도는 기하급수적으로 증가하고 있음\n",
    "# 언젠가는 우리가 만드는 프로그램이 사람의 지능을 앞서는 순간이 올 것이라 예측할 수 있음\n",
    "# 이 시점을 특이점(Singularity)이라고 함\n",
    "\n",
    "# 조만간 특이점이 올 것이라고 많은 사람들이 생각하고 있으며\n",
    "# 그 시점은 약 2045년이라고 예측하고 있음\n",
    "# 많은 학자들 중 일부는 특이점이 오는 시기가 인류가 멸망하는 시기라고 얘기 함\n",
    "# 회사들은 안전하다고 말함: 선악을 구별할 수 있을 것이기에\n",
    "#                        프로그래밍을 통해서 제어할 수 있을 것\n",
    "\n",
    "# 뇌과학자 => AI가 개발이되면 인공지능은 전자회로의 속도록 학습하고\n",
    "#            사람은 생화학적 회로로 학습함 // 약 100만 배의 차이\n",
    "#            100만배: 인공지능이 1주일 동안 할 수 있는 일을 MIT의 AI개발자들은 2만년 걸림\n",
    "\n",
    "# 현 시건 가장 빠른 슈퍼컴퓨터 => 미국 - IBM이 만든 서밋(summit)\n",
    "# 농구코드 두 배 정도되는 크기에 캐비넷 깔아놓고 그 안에 컴퓨터 채워놓음\n",
    "\n",
    "# 작년 10월 23일 구글이 네이처지에 양자컴퓨터 개발에 대한 내용을 발표함\n",
    "# 서밋이 1000년 동안 해야 하는 일을 3.5초만에 해결했음\n",
    "# IBM 주장: 2.5일 정도되는 일은 구글이 3.5초에 해결 한 것\n",
    "#          양자컴퓨터는 적용시킬 수 있는 특정 분야가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "cost value1: 0.7083088159561157\n",
      "cost value2: 0.6936931014060974\n",
      "cost value3: 0.693177342414856\n",
      "cost value4: 0.6931495666503906\n",
      "cost value5: 0.6931474208831787\n",
      "cost value6: 0.6931471824645996\n",
      "cost value7: 0.6931471824645996\n",
      "cost value8: 0.6931471824645996\n",
      "cost value9: 0.6931471824645996\n",
      "cost value10: 0.6931471824645996\n"
     ]
    }
   ],
   "source": [
    "# 인공지능(AI)\n",
    "# CS에서 궁극의 목표 중 하나\n",
    "# 1960년대부터 꾸준히 연구, 개발\n",
    "\n",
    "# 인간의 뇌를 연구하기 시작\n",
    "# 1958년에 perceptron을 모델링한 기계를 실제로 구현\n",
    "# 뉴욕타임즈에 기사가 실림\n",
    "# 조금만 있으면 스스로 말하고 듣고 쓰고 창조가 가능한 프로그램을 만들 수 있어요\n",
    "\n",
    "# AND/OR에 대한 logistic regression => perceptron\n",
    "# 진리표를 이용해서 학습할 것\n",
    "\n",
    "x_data=[[0,0],[0,1],[1,0],[1,1]]\n",
    "y_data=[[0],[1],[1],[0]]\n",
    "\n",
    "# placeholder\n",
    "X=tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y=tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# weight, bias\n",
    "W=tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b=tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit=tf.matmul(X,W)+b\n",
    "H=tf.sigmoid(logit)\n",
    "\n",
    "# cost\n",
    "cost=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                            labels=Y))\n",
    "\n",
    "# trian\n",
    "train=tf.train.GradientDescentOptimizer(learning_rate=.01).minimize(cost)\n",
    "\n",
    "# session & initializer\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# learning\n",
    "cnt=0\n",
    "for step in range(30000):\n",
    "    _, cost_val=sess.run([train, cost],feed_dict={X: x_data,\n",
    "                                                  Y: y_data})\n",
    "    if step%3000==0:\n",
    "        cnt+=1\n",
    "        print(f'cost value{cnt}: {cost_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict=tf.cast(H>.5, dtype=tf.float32)\n",
    "sess.run(predict, feed_dict={X:[[0,0],[0,1],[1,0],[1,1]]})\n",
    "\n",
    "# exclusive OR연산은 불가함 => 0 0 0 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron(logistic)으로 AND/OR는 구현이 가능\n",
    "# XOR(Exclusize OR)는 Perceptron으로 구현이 안 됨\n",
    "# 많은 사람들이 XOR를 어떻게 Perceptron으로 구현할 수 있을까?\n",
    "\n",
    "# 1969년에 마빈 민스키(MIT AI Lab의 창시자)라는 사람이 논문 하나 발표\n",
    "# XOR은 한 개의 Perceptron으로 학습이 불가능\n",
    "# MLP(Multi Layer Perceptron)으로는 가능함\n",
    "# MLP은 학습이 너무 어려워서 지구상에 있는 누구라도 이 학습을 시킬 수 없음\n",
    "# AI 망함\n",
    "\n",
    "# 1974년도 Paul이라는 박사과정 학생이 Backpropagation 방법 고안\n",
    "# 늦어서 묻힘\n",
    "# 1982년도에 다시 한 번 논문 발표, 다시 묻힘\n",
    "# 1986년도에 Hinton 교수님이 논문 발표\n",
    "# 활활 타오름, AI가 각광을 받기 시작\n",
    "\n",
    "# 1995년 즈음 Backpropagation 방식이 안되는건 아닌데\n",
    "# 입력이 더 복잡한 데이터를 학습하는 것은 역시나 해결이 불가능함\n",
    "# 이 시대에 다른 여러가지 알고리즘이 나타나기 시작\n",
    "# SVM, 나이브 베이지언, Decision Tree etc\n",
    "# LeCUN교수 => 다른 알고리즘이 더 우수하다는 것을 증명\n",
    "# 다시 침체기에 들어감\n",
    "\n",
    "# 캐나다에서 국책 연구기관을 설립\n",
    "# Canadian lnstitute For Advanced Research(CIFAR)\n",
    "# 1987년에 Hinton교수 캐나다로 건너가서 AI 연구 지속\n",
    "# 2006, 2007년도에 2개의 논문 발표\n",
    "# 망했던 이유를 찾음\n",
    "# 2006년도 논문 => W,b의 초기값을 random으로 주면 안됌\n",
    "# 2007년도 논문 => 초기값 증명에 대한 논문, Layer를 더 많이 사용할수록 복잡한 문데를 해결할 수 있어요\n",
    "\n",
    "# 사람들의 반응이 없음 => deep learning으로 rebranding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost value1: 4.258972644805908\n",
      "cost value2: 0.0014836040791124105\n",
      "cost value3: 0.0007295273244380951\n",
      "cost value4: 0.0004773418768309057\n",
      "cost value5: 0.000352222821675241\n",
      "cost value6: 0.00027797071379609406\n",
      "cost value7: 0.0002290690754307434\n",
      "cost value8: 0.00019457945018075407\n",
      "cost value9: 0.00016901314666029066\n",
      "cost value10: 0.00014934703358449042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data=[[0,0],[0,1],[1,0],[1,1]]\n",
    "y_data=[[0],[1],[1],[0]]\n",
    "\n",
    "# placeholder\n",
    "X=tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y=tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# weight, bias\n",
    "# 10개의 node?\n",
    "W1=tf.Variable(tf.random_normal([2,100]), name='weight1')\n",
    "b1=tf.Variable(tf.random_normal([100]), name='bias1')\n",
    "\n",
    "layer1=tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([100,256]), name='weight2')\n",
    "b2=tf.Variable(tf.random_normal([256]), name='bias2')\n",
    "\n",
    "layer2=tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3=tf.Variable(tf.random_normal([256,1]), name='weight2')\n",
    "b3=tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "\n",
    "# Hypothesis\n",
    "logit=tf.matmul(layer2,W3)+b3\n",
    "H=tf.sigmoid(logit)\n",
    "\n",
    "# cost\n",
    "cost=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                            labels=Y))\n",
    "\n",
    "# trian\n",
    "train=tf.train.GradientDescentOptimizer(learning_rate=.1).minimize(cost)\n",
    "\n",
    "# session & initializer\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# learning\n",
    "cnt=0\n",
    "for step in range(30000):\n",
    "    _, cost_val=sess.run([train, cost],feed_dict={X: x_data,\n",
    "                                                  Y: y_data})\n",
    "    if step%3000==0:\n",
    "        cnt+=1\n",
    "        print(f'cost value{cnt}: {cost_val}')\n",
    "        \n",
    "predict=tf.cast(H>.5, dtype=tf.float32)\n",
    "sess.run(predict, feed_dict={X:[[0,0],[0,1],[1,0],[1,1]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
